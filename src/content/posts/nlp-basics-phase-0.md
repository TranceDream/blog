---
title: '前端小猫也能看懂的 NLP 入门第0章 - 前置知识'
published: 2026-01-13
draft: false
description: '前端小猫如何入门 NLP ？请看这里，本文章不教模型，不提算法，从基础知识点切入，让你快速上手自然语言处理~'
series: 'NLP Basics'
tags: ['NLP']
---

# 学习NLP，你都需要了解什么

我发现，很多人在学习 NLP 时，都会陷入这样的困境：NLP 领域名词实在是太多，文法语义、甚至还有变形金刚(transformer)，贸然去学习只会一头雾水，随便点开一些文献阅读，里面大量的复杂公式，看得人头皮发麻，学习起来完全无从下手。所以我准备编写这样一组文章，从最基础的概念开始，不上复杂的数学概念，以能够理解模型在做什么为目标，介绍 NLP 的各种知识。

:::pusheen{align=left}
现在，我们先了解一下什么是 NLP 。
:::

## NLP 到底在做什么

NLP 并不是要让机器用人类的方式去理解语言，而是使用数学的方式，将文本转换成数学对象，在数学空间里进行计算。几乎所有的 NLP 系统，无论是搜索、分类、翻译，还是 LLM，都离不开“文本 -> 数字 -> 向量 -> 计算 -> 结果”的流程。所以可以说那些看似复杂的模型，其实都是这个流程的不同实现形式。

## 向量是什么

在代数上，一个向量(Vector)的定义是一个有序数字的列表，如$(x, y)$，顺序不可变。一个 n 维的向量可以表示为$\mathbf{v} = (v_1, v_2, \dots, v_n)$。在 NLP 领域，向量可以理解为一个对象在某个“特征空间”中的坐标。其中每一个$v_i$并不是人类可以解释的含义，而是模型在训练中自动学习到的特征维度。

一旦文本被映射到向量空间，我们就可以对它进行**度量**(计算距离/相似度)、**组合**(作加权/投影)和**变换**(线性/非线性变换)，这几种能力恰恰是机器学习所擅长的。

## 相似度：如何用数学度量

其实在 NLP 中，大部分的问题都可以解释为“在向量空间里面，哪些点彼此更接近”。比如：

- 搜索问题：查询向量和文档向量的距离
- 分类问题：向量是否落在某个区域中

相似度的计算正是来回答这些问题的。

在 NLP 的工程实践中，向量的长度往往并不是我们要关心的，向量的方向代表的是语义，也就是我们要关注的点。我们通常计算**余弦相似度**来判断两个向量的方向相似性。余弦相似度衡量的是两个向量夹角的余弦值。

$$
\text{cosine}(\mathbf{a}, \mathbf{b}) =
\frac{\mathbf{a} \cdot \mathbf{b}}{||\mathbf{a}|| , ||\mathbf{b}||}
$$

看下面的图会稍微直观一些，以二维的向量举例，几个箭头分别代表我们存储的几个词语的向量，现在我们用一个关键词“狗”去查询，生成一个“狗”向量(黄色的那条)，于是我们就能从我们的数据中看出来，明显绿色箭头的“狗粮”方向要更加接近，这样就能实现一个简单的语义搜索系统。

[TODO] 插图

## 从概率的视角了解 NLP

从统计建模的角度，绝大多数 NLP 模型在学习的是$P(y \mid x)$，也就是“在输入一个词x的情况下，输出y出现的概率”。更通俗的来说，就是通过上一个词猜测下一个词最可能是什么词。这就意味着，模型输出的并不是唯一答案，而是概率分布。

实际上，现在的模型并不是通过一个词去猜测下一个词，而是通过多个词组成的上下文，公式就扩展为$P(w_t \mid w_1, w_2, \cdots ,w_{t-1})$。这样看下来，LLM 生成文本其实是概率链式展开的结果。也是因此，模型看起来似乎懂得语义，但是实际上不能真正准确的理解世界。

## 关于梯度

在数学上，梯度(Gradient)是指“多元函数在某一个点处，增长最快的方向”。

:::pusheen{align=right}
想象一下你站在一个山谷中，你在你附近一圈的脚下试探一下，就可以知道哪个方向最陡，哪个方向的地势下降越快。
:::

在机器学习中，有一个概念是**损失函数(Loss Function)**，它回答的是“现在的模型有多差”，而它的变量也就是模型的参数。一个损失函数$L(\theta_1, \theta_2, \dots, \theta_n)$，它的梯度是一个向量$\nabla L =
\left(
\frac{\partial L}{\partial \theta_1},
\frac{\partial L}{\partial \theta_2},
\dots,
\frac{\partial L}{\partial \theta_n}
\right)$，这个向量所表示的就是每个参数向哪个方向动一点点，Loss 会涨得最快，模型变差就最快。那么如果我们向它的反方向调整参数，模型就会变得更好，这就是**梯度下降(Gradient Descent)**的概念。

说回 NLP 领域，这里的参数就是成千上万的数据(embedding、权重)，Loss 就是预测错词/分类错误的惩罚，梯度就是参数如何调整，才能让预测更加准确。

## 最后

大部分的概念只需要了解它是做什么的，具体原理和数学公式不需要深究。走到这一步，你已经可以正式入门 NLP 世界了。下一章我会讲讲最传统的 NLP 是怎么工作的。
