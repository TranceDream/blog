---
title: '前端小猫也能看懂的 NLP 入门第1章 - 传统 NLP'
published: 2026-01-30
draft: false
description: '最开始的时候，没有词向量，没有 embedding，NLP 是什么样的？'
series: 'NLP Basics'
tags: ['NLP']
---

在第0章我们说到，计算机只能处理数字，而不能直接处理文本，那么我们是如何将文本转换成数字的呢？在传统的 NLP 时代，人们对文本的认知就是“一串**词**组成的序列”，所以最开始，人们把**词**作为处理的最基础单位。

## 文本预处理

处理文本，分词是第一道门槛。英文分词相对来说比较简单，英文使用空格、标点符号、和大小写的区分就能天然的把词提取出来。而中文没有天然的分隔符，例如“我们研究生命的起源”，其实就可以分成“我们/研究/生命/的/起源”和“我们/研究生/命/的/起源”。对于中文的分词，学者们研究了很多分词方法，包括基于词表的**最大匹配法**、基于统计模型的 **N-gram 分词方法**、基于序列标注的分词方法等，这里我就不一一说明了，感兴趣可以自行搜索。

在分词时，还有一些处理步骤，比如**去停用词**。指的是“的、了、是、the、is、and”这种出现频率极高，但是对语义的区分度比较低的词，在分词的时候我们要排除这些词的影响。这一步虽然在现在来看似乎存在很大的问题，但是在传统 NLP 时期这是非常合理的做法。同样是为了减少处理的复杂度，在英文中会把**词性还原**，如“run、runs、running、ran”在传统 NLP 中会映射为同一个词根。

## Bag of words

词袋模型 (Bag of Words，简称 BoW) 是传统 NLP 时代最经典、也最朴素的文本表示方法。

### 什么是词袋模型

它的核心思想可以用简单的一句话来总结：**我们不关心词的顺序，只关心有哪些词出现了、出现了几次。**

你可以把一段文本想象成一个“袋子”，把里面出现过的词都丢进去。至于他为什么叫“词袋”，因为它真的像一个袋子：袋子里装的是词，袋子不记录顺序，袋子只记录数量。

:::pusheen{align=right}
举个例子：“我 打 你”和“你 打 我”。从正常人类角度看，这两句话的意思完全不同，但在 BoW 的视角里，它们的袋子内容是一样的：`我: 1     打: 1       你: 1`。所以 BoW 会认为它们是一样的。
:::

这也是传统 NLP 的一个典型特点：**为了可计算性，必须先牺牲掉一部分语言的结构信息。**

### BoW 如何把文本变成向量？

BoW 的做法其实非常工程化：

1. 先统计整个语料库里出现过的所有词，做成一个**词表 (vocabulary)**
2. 给词表里的每个词分配一个维度
3. 一段文本就变成一个向量：这个词出现过 → 对应维度 +1（或置为 1），没出现 → 对应维度 = 0

比如我们的词表是：`["我", "喜欢", "自然", "语言", "处理"]`，那么文本“我 喜欢 自然 语言 处理”就可以表示成（按出现次数）：`[1, 1, 1, 1, 1]`。而文本：“我 喜欢 处理”，就成了：`[1, 1, 0, 0, 1]`。文本就这样被翻译成了数学对象。

### BoW 的优点

BoW 虽然看起来十分粗糙吧，但是它在某些领域上其实表现的出奇得好，比如：

* 文本分类（比如垃圾邮件识别）
* 简单检索（关键词搜索）
* 主题粗分类（大概知道在讲什么）

原因很简单：很多任务并不需要精确地理解句子的含义，只需要知道句子里有没有出现某些词语。

### BoW 的缺点

BoW 的问题也很明显：

- 维度爆炸：现实语料的词表规模动辄上万词，在 BoW 下就表现为一个巨大的高维度稀疏向量，大部分位置都是0，只有少数位置非0。
- 不懂语义：非常经典的例子，“再见”和“拜拜”，这显然是同一个意思，但是在 BoW 中是两个不同的维度。
- 不关心顺序：“我打你”和“你打我”，在 BoW 下反而会更加相似，因为它不会处理文本的顺序。

:::handshake-pusheen{align=right}
为了解决 BoW 的部分缺点，我们有了 TF-IDF
:::

## TF-IDF

TF-IDF (Term Frequency - Inverse Document Frequency)，它的理念是让重要的词更重要，让到处都有的词更不重要。这是什么意思呢，我们得把 TF 和 IDF 拆开看。

### TF (Term Frequency)

TF，也就是词频，代表这个词在当前文本里出现了几次，出现越多就越可能是重点。比如一篇关于猫粮测评的文章里，“猫粮”出现了30次，那它大概率是主题词。

### IDF (Inverse Document Frequency)

光看 TF 也不够，因为好多话佐料也会频繁出现在文章中，如“嗯哎这是”这种词，但是这并不代表他们就是重要的。所以 IDF 引入了一个全局视角，如果一个词在所有文档里都很常见，那它大概率不是一个很重要的词。反而一个词在少数文档里频繁出现，那它就很可能是能区分文本主题的关键词。

### TF-IDF：把两种直觉结合起来

最后 TF-IDF 做的事就是**TF (局部重要) × IDF (全局稀有)**，也就是**“在这篇文章里经常出现、但在别的文章里不常见的词，最有信息量”**。

这一特点使得 TF-IDF 更擅长关键词提取、文本检索、文本分类的任务。

### 语义问题仍未解决

TF-IDF 本质上仍然是词统计，只是它更加聪明了，所以它仍然无法解决理解语义的问题。

## 小结

* 传统 NLP 能做什么：分类、检索、关键词
* 传统 NLP 做不到什么：真正的语义理解、同义表达、上下文推理

下一章我们来了解一下词向量和 embedding，看看语义理解的问题是如何解决的。
