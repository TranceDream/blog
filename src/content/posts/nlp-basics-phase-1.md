---
title: '前端小猫也能看懂的 NLP 入门第1章 - 传统 NLP'
published: 2026-01-14
draft: true
description: '最开始的时候，没有词向量，没有 embedding，NLP 是什么样的？'
series: 'NLP Basics'
tags: ['NLP']
---

在第0章我们说到，计算机只能处理数字，而不能直接处理文本，那么我们是如何将文本转换成数字的呢？在传统的 NLP 时代，人们对文本的认知就是“一串**词**组成的序列”，所以最开始，人们把**词**作为处理的最基础单位。

## 文本预处理

处理文本，分词是第一道门槛。英文分词相对来说比较简单，英文使用空格、标点符号、和大小写的区分就能天然的把词提取出来。而中文没有天然的分隔符，例如“我们研究生命的起源”，其实就可以分成“我们/研究/生命/的/起源”和“我们/研究生/命/的/起源”。对于中文的分词，学者们研究了很多分词方法，包括基于词表的**最大匹配法**、基于统计模型的 **N-gram 分词方法**、基于序列标注的分词方法等，这里我就不一一说明了，感兴趣可以自行搜索。

在分词时，还有一些处理步骤，比如**去停用词**。指的是“的、了、是、the、is、and”这种出现频率极高，但是对语义的区分度比较低的词，在分词的时候我们要排除这些词的影响。这一步虽然在现在来看似乎存在很大的问题，但是在传统 NLP 时期这是非常合理的做法。同样是为了减少处理的复杂度，在英文中会把**词性还原**，如“run、runs、running、ran”在传统 NLP 中会映射为同一个词根。

## Bag of words

词袋模型(Bag of Words)